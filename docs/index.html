<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Guía Profesional: TensorFlow GPU, Docker y VS Code</title>
  <style>
    * { box-sizing: border-box; }
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji";
      line-height: 1.6; background: #f6f8fa; color: #24292e; margin: 0;
    }
    .container {
      width: 90%; max-width: 1000px; margin: 30px auto; padding: 36px;
      background: #fff; border: 1px solid #d1d5da; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,.05);
    }
    h1 { font-size: 2.1em; border-bottom: 1px solid #eaecef; padding-bottom: .3em; margin-top: 0; }
    h2 { font-size: 1.6em; border-bottom: 1px solid #eaecef; padding-bottom: .3em; margin-top: 1.6em; }
    h3 { font-size: 1.25em; margin-top: 1.2em; }
    code:not(pre code) {
      background: #f3f3f3; padding: .15em .35em; border-radius: 4px; font-family: Menlo, Consolas, monospace; font-size: .9em;
    }
    pre {
      background: #282c34; color: #abb2bf; padding: 1em; border-radius: 6px; overflow-x: auto;
      font-family: Menlo, Consolas, "Liberation Mono", monospace; font-size: .9em; line-height: 1.45;
    }
    pre code { background: transparent; color: inherit; font-family: inherit; }
    ul { padding-left: 22px; }
    table { width: 100%; border-collapse: collapse; margin: 1em 0; font-size: .95em; }
    th, td { border: 1px solid #dfe2e5; padding: 8px 12px; text-align: left; }
    thead th { background: #f6f8fa; }
    .note { background: #fff8db; border: 1px solid #ffe58f; padding: 10px 12px; border-radius: 6px; }
  </style>
</head>
<body>
  <main class="container">
    <h1>Guía profesional: TensorFlow GPU + Docker + VS Code</h1>

    <div class="note">
      Separación clave:
      <ul>
        <li><strong>devcontainer.json</strong>: entorno de desarrollo (monta tu carpeta y GPU; no copia datos).</li>
        <li><strong>Dockerfile</strong>: imagen para ejecución/producción (no incluye datasets/modelos grandes).</li>
        <li><strong>.dockerignore</strong>: excluye data/models y artefactos del contexto de build.</li>
      </ul>
    </div>

    <h2>1) Estructura final del proyecto</h2>
    <pre><code>tf_gpu_guide/
├─ Dockerfile
├─ .dockerignore
├─ .gitignore
├─ README.md
├─ docs/
│  └─ guide.html
├─ .github/
│  └─ workflows/
│     └─ docker-ci-cd.yml
├─ .devcontainer/
│  └─ devcontainer.json
├─ requirements/
│  ├─ base.txt          # deps de desarrollo y utilidades (sin TF)
│  ├─ dev.txt           # extiende base (ipykernel, pytest)
│  ├─ prod-no-tf.txt    # freeze filtrado para imagen (sin pila TF ni dev)
│  └─ tf-lock.txt       # snapshot completo (pip freeze)
├─ scripts/
│  ├─ freeze_env.sh     # genera prod-no-tf.txt desde tf-lock.txt
│  └─ build_and_push.sh # ejemplo push multi-cloud (opcional)
├─ src/
│  ├─ train_model.py    # entrenamiento: guarda en ${MODEL_DIR}/model.keras
│  └─ inference.py      # API FastAPI para servir el modelo
├─ data/                # datos locales (desarrollo; excluidos del build)
│  └─ sample/           # opcional: pequeño dataset para demos/tests
├─ models/              # modelos guardados (local / volumen en prod)
└─ notebooks/
   └─ example.ipynb
</code></pre>

    <h2>2) Desarrollo en VS Code (Dev Container)</h2>
    <ul>
      <li>Abrir: Command Palette → “Dev Containers: Reopen in Container”.</li>
      <li>Imagen: <code>tensorflow/tensorflow:2.16.1-gpu</code>. Montaje: <code>${localWorkspaceFolder} → /app</code>. GPUs: <code>--gpus all</code>.</li>
      <li>Deps de dev: se instalan con <code>postCreateCommand</code>: <code>pip install -r requirements/dev.txt</code>.</li>
    </ul>
    <h3>Verificar GPU y TF</h3>
    <pre><code>python -c "import tensorflow as tf; print('TF:', tf.__version__); print('GPUs:', tf.config.list_physical_devices('GPU'))"
# opcional
nvidia-smi
</code></pre>

    <h3>Entrenar y guardar el modelo</h3>
    <pre><code># guarda en /app/models/model.keras (persistente en tu host)
python src/train_model.py
ls -la models</code></pre>

    <h3>Probar la API de inferencia en desarrollo</h3>
    <pre><code># terminal 1 (devcontainer)
python -m uvicorn src.inference:app --host 0.0.0.0 --port 8000

# terminal 2 (host o devcontainer)
curl http://localhost:8000/health
curl -X POST http://localhost:8000/predict -H "Content-Type: application/json" -d "[1,2,3,4]"</code></pre>

    <h2>3) Imagen de ejecución/producción (Dockerfile)</h2>
    <ul>
      <li>Base: <code>tensorflow/tensorflow:2.16.1-gpu</code> (no instalar TF por pip).</li>
      <li>Instala <code>requirements/prod-no-tf.txt</code>.</li>
      <li>Copia solo <code>src/</code> y (si quieres) <code>data/sample/</code>.</li>
      <li>En producción monta <code>models/</code> como volumen o descarga del cloud al arranque.</li>
    </ul>
    <pre><code># build (Windows PowerShell en la raíz)
docker build -t tf-gpu-infer .

# run sirviendo el modelo entrenado (monta ./models)
docker run --rm -it --gpus all -p 8000:8000 -v "${PWD}/models:/app/models" tf-gpu-infer

# probar
curl http://localhost:8000/health
curl -X POST http://localhost:8000/predict -H "Content-Type: application/json" -d "[1,2,3,4]"</code></pre>

    <h2>4) CI: Build + smoke test (sin push)</h2>
    <ul>
      <li>Workflow: <code>.github/workflows/docker-ci-cd.yml</code>.</li>
      <li>Construye imagen, verifica TensorFlow dentro del contenedor y sube <code>image_*.tar</code> como artifact.</li>
    </ul>
    <pre><code># Smoke test dentro del job:
docker run --rm &lt;img&gt; python -c "import tensorflow as tf; print('TF', tf.__version__); print('GPUs', tf.config.list_physical_devices('GPU'))"</code></pre>

    <h2>5) Publicar/servir en cloud (resumen)</h2>
    <ul>
      <li>Push al registry (ECR/ACR/Artifact Registry) → desplegar en AKS/EKS/GKE/VM con GPU.</li>
      <li>Montar/inyectar el modelo: volumen, init container o descarga en startup desde bucket.</li>
      <li>Monitorizar latencia y calidad; re-entrenar bajo demanda.</li>
    </ul>

    <h2>6) Renderizar esta guía en GitHub</h2>
    <p><strong>Opción A (recomendada): GitHub Pages</strong> — no necesitas convertir a Markdown.</p>
    <ol>
      <li>Settings → Pages → “Build and deployment”: Source = “Deploy from a branch”.</li>
      <li>Branch = <code>main</code>, Folder = <code>/docs</code> → Save.</li>
      <li>Accede a: <code>https://&lt;tu-usuario&gt;.github.io/&lt;repo&gt;/guide.html</code> (o crea un <code>docs/index.html</code>).</li>
    </ol>
    <p><strong>Opción B</strong> — convertir a Markdown si solo quieres verlo renderizado dentro del repo (GitHub renderiza MD por defecto). No es necesario si usas Pages.</p>

    <h2>7) Troubleshooting rápido</h2>
    <ul>
      <li><em>GPUs=[]</em>: habilitar GPU en Docker Desktop; usar backend WSL2; drivers NVIDIA actualizados.</li>
      <li><em>Build falla por requirements</em>: ejecuta <code>bash scripts/freeze_env.sh</code> en el devcontainer y reintenta.</li>
      <li><em>Modelo no encontrado</em>: verifica que <code>models/model.keras</code> exista y que el volumen esté montado.</li>
    </ul>
  </main>
</body>
</html>